-ç»ƒä¹ å¦‚ä½•å›ç­” System Design é¢˜ç›®

-é¢è¯•æ—¶å¯ä»¥æŒ‰ç…§ 4-6-2 åŸåˆ™ è¿›è¡Œå›ç­”ï¼š
- 4 åˆ†é’Ÿï¼šéœ€æ±‚åˆ†æ
    Clarify éœ€æ±‚ï¼Œç¡®è®¤æ˜¯å¦æœ‰ SLAã€QPS éœ€æ±‚
    è¯†åˆ«æ ¸å¿ƒåŠŸèƒ½å’ŒéåŠŸèƒ½æ€§è¦æ±‚
- 6 åˆ†é’Ÿï¼šæ¶æ„è®¾è®¡
    ç”»å‡ºæ•´ä½“æ¶æ„ (å®¢æˆ·ç«¯ â†’ è´Ÿè½½å‡è¡¡ â†’ æœåŠ¡å±‚ â†’ æ•°æ®å±‚)
    è§£é‡Šå„ä¸ªç»„ä»¶çš„ä½œç”¨
    è®¨è®ºå¯æ‰©å±•æ€§ã€å®¹é”™æ€§
- 2 åˆ†é’Ÿï¼šä¼˜åŒ– & å–èˆ
    è®¨è®º Trade-offs (æ•°æ®ä¸€è‡´æ€§ vs å¯ç”¨æ€§)
    æ–¹æ¡ˆå¯¹æ¯” (SQL vs NoSQL, Monolithic vs Microservices)
- ä¸€å¹´ æ˜¯ 30  million ç§’
- ä¸€å¤© æ˜¯ 100 k ç§’

- Template
  - Requirements
    - Functional requirements
    - Non-Functional requirements
  - Entities
  - APIs 
  - [High-Level Design](./high_level_design.png)
  - Deep Dive

- 4. RATE LIMITER
  counting / sliding window / redis hash {userID: (count, startTime)} for 1 hour / consistent hashing / different rate limiters for different APIs
  - purpose
    - DDOS
    - lower the cost
    - VIP for higher rate
    - spike traffic elimination
  - Requirements
    - Functional requirements 
      - 15 requests/second user
      - conpatible with cluster settings
    - Non-Functional requirements 
      - high availability (protect from attack)
      - low latency (introduce low or no latency)
  - Entities
    - User
    - sliding window
  - APIs
    - GET  resources/ -> 429 too many requests
    - POST resources/ -> 429
  - [Design](./4.high_level_design.png)
    - Throttling (http 429 too many requests)
      - hard
      - soft
      - dynamic
    - algo1: counting {userID: (count, startTime)}
      CurrentTime â€“ StartTime >= 1 min
      - Atomicity: read-and-then-writeâ€ behavior can create a race condition
        - two requests at the same time can get the same count
        - redis servers with lock in a distributed setup
      - alow twice the limit
      - Mem usage
        userID 8 bytes
        counter 2 bytes
        startTime 2 bytes only store minute and second
        1 million users
        hash table overhead 20 bytes/record
        lock overhead 4 bytes number lock
        1 million * (8+2+2+20+4)bytes = 36 MB
        
    - algo2: sliding window
      - redis sorted set {userID: SortedSet<Time>}
        8 (userID) + (4 (time) + 20 (sorted set overhead)) * 500 request/hour + 20 (hash-table overhead) = 12KB
        * 1 million = 12GB
      - expensive
    - algo3: 1+2
      1 min counter (count, startTime) using Redis Hash and set expire in 1 hour
      at most 60 counters/hour
      8 + (4 + 2 + 20 (Redis hash overhead)) * 60 + 20 (hash-table overhead) = 1.6KB
      * 1 million = 1.6GB
  - Data Sharding and Caching
    - consistent hashing by userID
    URL Shortener; we can have different rate limiter for createURL() and deleteURL() APIs for each user or IP.
    - cache write back later
  - limit by ID or IP
    - IP bad user can limit other good users in the same network
      IPv6 huge number from even one computer
    - ID
      - Login Auth bad user can enter bad password for the good user DoS
    - IP + ID
      - more memory and storage  

- 6. KEY-VALUE STORE : Dynamo
  Partition | Replication | Consistency(Conflicts) | Scailability | Cluster state
  - CAP ä¸­çš„ A (Available) P (Partition Tolerance)
  - APIs
    - get(key) -> object && context
    - put(key, context, object)
      The context is always stored along with the object and is used like a cookie to verify the validity of the object supplied in the put request.
  - Data partition
    - [[consistent hashing]] with virtual nodes
      randomly distributed and non-contiguous
      - evenly distributed && rebalancing fast
      - heterogeneous machines
        lower ranges for less powerful machines
      - no hot spots
  - Replication
    - replication factor 
    - hinted handoff
      æœåŠ¡å™¨æŒ‚äº†ä¹‹åï¼Œreplication é¡ºå»¶ with hint
    - Sloppy quorum
      æ˜¯ä¸€ä¸ªå®¹é”™å†™å…¥æœºåˆ¶ã€‚
      æŒ‡çš„æ˜¯å½“ä¸€éƒ¨åˆ†èŠ‚ç‚¹ä¸å¯ç”¨æ—¶ï¼Œå†™å…¥æ“ä½œå¯ä»¥è¢«æš‚æ—¶å­˜å‚¨åœ¨â€œä¸´æ—¶â€èŠ‚ç‚¹ä¸Šï¼Œç¨åå†åŒæ­¥å›æ­£ç¡®èŠ‚ç‚¹ã€‚
      ç›®çš„æ˜¯æé«˜ç³»ç»Ÿçš„å¯ç”¨æ€§
    - [conflict scenario](./6.Conflict_scenario.png)
  - Vector clock and conflicting data
    - server A and server B
      {A:1} {A:2} {B:1} we only know {A:1} is older
      {A:2} and {B:1} need to be resolved by client
      æˆ–è€… merge {A:2} å’Œ {B:1} çš„ object
    - Conflict-free replicated data types
    - Last-write-wins
  - data reconciliation
    - Merkle trees, solve the conflict in the background
  - Cluster states
    - gossip protocol and [seed nodes] (know all other nodes)
  - follow up: å¦‚ä½•å¤„ç† TTLï¼ˆTime-To-Liveï¼‰çš„åˆ é™¤
    - Lazy Delete + Cron Job
  


- 7. UNIQUE ID GENERATOR IN DISTRIBUTED SYSTEMS
  - éœ€æ±‚åˆ†æ: 
    - unique
    - 64bit
    - 10000 IDs/second
    - increasing by time
  - databases auto_increment
    - multiple data centers ç½‘ç»œå»¶è¿Ÿ æ•°æ®ä¸ä¸€è‡´
    - time å¯èƒ½ä¸æ˜¯æ¥è‡ªåŒä¸€ä¸ªæœåŠ¡å™¨ï¼Œä¸ä¸€å®šæ˜¯é€’å¢çš„
    - æ·»åŠ æœåŠ¡å™¨å‡å°‘æœåŠ¡å™¨éƒ½éš¾åŠ
  - Twitter snowflake approach
      |  41 bits  |    5 bits     |   5 bits   |     12 bits     |
      |-----------|---------------|------------|-----------------|
      | Timestamp | Datacenter ID | Machine ID | Sequence number |
      | Millisecs |  32 centers   | 32 machine |reset millisecond|








- 8. URL SHORTENER
  - éœ€æ±‚åˆ†æ: 
    100 million/day
    write/second = 1160
    read         = 11600
    10 years records     = 100 million/day * 365 * 10 years = 365 billion
    storage for 10 years =  365 billion * 100 bytes = 36.5 TB
    values in short URLs: 0 a A = 62 chars
    length of short URLs: 7 
    - Functional Requirementsï¼ˆåŠŸèƒ½æ€§éœ€æ±‚ï¼‰
      1. URL ç¼©çŸ­:   ç”¨æˆ·æä¾›ä¸€ä¸ª long URLï¼Œç³»ç»Ÿè¿”å›ä¸€ä¸ªå”¯ä¸€çš„ short URLã€‚
      2. URL é‡å®šå‘: è¾“å…¥çŸ­é“¾æ¥ï¼Œç³»ç»Ÿèƒ½å¤Ÿæ­£ç¡®æŸ¥æ‰¾åˆ°å¯¹åº”çš„é•¿é“¾æ¥å¹¶é‡å®šå‘ã€‚
      3. å”¯ä¸€æ€§ä¿è¯:  å¯¹äºæ¯ä¸€ä¸ª long URLï¼ˆæˆ–ç”¨æˆ·è¯·æ±‚ï¼‰ï¼Œç”Ÿæˆçš„ short URL æ˜¯å”¯ä¸€çš„ï¼Œé¿å…å†²çªã€‚
    - Non-Functional Requirements
      - Scalability ï½œ High Availability ï½œ Low Latency
  - Entities
    URL mapping
    User
  - APIs
    - POST /shorten/{longURL=?} -> shortURL
    - GET  /{shortURL}          -> longURL
  - [URL Shortening POST](./8.URL%20Shortening.png)
    - Database format:
      |    ID   | Short URL | Long URL 
      |---------|-----------|----------
      | 12345   |  zn9edcu  | https://en.wikipedia.org/wiki/Systems_design
      | 56644   |  fg8723j  | https://en.wikipedia.org/wiki/Systems
    - hash
      - collision
        - Bloom Filter: if element is a member of the set
    - BASE 62 on unique ID (not on the long URL)
      - not fixed len
      - ID generator [Chapter 7: Design A Unique ID Generator in Distributed
Systems]
      - unique
  - [URL Redirecting (GET)](./8.URL%20Redirecting.png)
  - follow up 
    - Analytics
    - Database scaling
    - Rate limiter (Chapter 4: Design a rate limiter)

- 9. WEB CRAWLER
  - Requirements
    - Functional
      - Crawl the full web starting from seed urls
      - Extract text data and store
    - Non-functional
      - Fault tolerance 
      - Politeness
      - Scale to 10B pages
      - Efficient to crawl in 5 days
  - API or System Interface
    - Input: Seed URLs to start crawling from.
    - Output: Text data extracted from web pages.
  - Data Flow
    1. Take seed URL from frontier and request IP from DNS
    2. Fetch HTML from external server using IP
    3. Extract text data from the HTML.
    4. Store the text data in a database.
    5. Extract any linked URLs from the web pages and add them to the list of URLs to crawl.
    6. Repeat steps 1-5 until all URLs have been crawled.
  - [High Level Design](./9.high_level_design.png)
  - identify bottleneck 
    - wait for web response
    - saparate crawl and parse
  - [improvement](./9.improvement.png)
    - crawl (distributed DNS/crawling) (DNS cache)
    - parse (on-demand)
  - failed URL retry / backoff 1,2,4,8 seconds
  - [[MATH]] how many crawlers do we need?
    - 10B pages / 5 days = 2000 pages/second
    - 1 crawler can crawl 100 pages/second
    - 20 crawlers
  - å»é‡
    - URL å»é‡
      - hash table
    - å†…å®¹å»é‡
      - checksum
    - bloom filter

- 10. NOTIFICATION SYSTEM
  - Requirements
    - Functional
      - å¤šæ¸ é“é€šçŸ¥æ”¯æŒï¼š
        ç³»ç»Ÿåº”æ”¯æŒé€šè¿‡å¤šç§æ¸ é“å‘é€é€šçŸ¥ï¼Œå¦‚ Emailã€SMSã€Push Notificationã€In-App ç­‰ã€‚
      - é€šçŸ¥æŠ•é€’çŠ¶æ€è¿½è¸ªï¼š
        ç³»ç»Ÿåº”èƒ½è®°å½•é€šçŸ¥æ˜¯å¦æˆåŠŸå‘é€ã€å¤±è´¥åŸå› ï¼Œä»¥åŠç”¨æˆ·æ˜¯å¦é˜…è¯»ï¼ˆå¯é€‰ï¼Œå–å†³äºæ¸ é“ï¼‰ã€‚
    - Non-functional
      - é«˜å¯ç”¨æ€§ï¼ˆHigh Availabilityï¼‰ï¼š
        é€šçŸ¥ç³»ç»Ÿåº”è®¾è®¡æˆé«˜å¯ç”¨æ¶æ„ï¼Œç¡®ä¿ä¸ä¼šå› ä¸ºéƒ¨åˆ†æœåŠ¡å®•æœºå¯¼è‡´æ•´ä¸ªé€šçŸ¥é“¾ä¸­æ–­
      - å¯æ‰©å±•æ€§ï¼ˆScalabilityï¼‰ï¼š
        ç³»ç»Ÿéœ€è¦æ”¯æŒæ°´å¹³æ‰©å±•ï¼Œèƒ½åœ¨é«˜å³°æœŸå¤„ç†æ•°ç™¾ä¸‡æ¡é€šçŸ¥è¯·æ±‚ï¼ˆæ¯”å¦‚é»‘äº”ã€åŒåä¸€æ´»åŠ¨æ—¶ï¼‰ã€‚
      - ä½å»¶è¿Ÿï¼ˆLow Latencyï¼‰ï¼š
        ç³»ç»Ÿåº”èƒ½ä¿è¯é€šçŸ¥åœ¨äº‹ä»¶å‘ç”Ÿåå°½å¿«ï¼ˆä¾‹å¦‚ <1 ç§’å†…ï¼‰é€è¾¾ç”¨æˆ·ï¼Œå°¤å…¶æ˜¯å¯¹ Push/In-App é€šçŸ¥ã€‚
  - Entities
    User: user_id, device_id, phone, email
    Notification: notification_id, user_id, type, content
  - APIs
    - POST /notifications/send
      - body: {user_id, type, content}
      - response: {status, notification_id}
    - GET /notifications/status/{notification_id} -> {status, error_message}
  - [High Level Design](./10.high_level_design.png)
    - Cache: User info, device info, notification templates are cached.
    - DB: It stores data about user, notification, settings, etc.
  - [Rate limiting / Retry mechanism / Events tracking (clicks, opens)](./10.improved_design.png)

- 1-11. NEWS FEED SYSTEM
  - Requirements
    - Functions
      - Create Posts
      - Follow people
      - View feed
    - Non-Functional Reuquirements
      - Posts visible in < 2 minutes (eventually consistent)
      - Posting and viewing posts < 500ms
      - Massive number of users 2B
      - Unlimited followers/follows
  - Entities
    User
    Post
    Follow
    // added afterward feed: {uid : [post_id1, post_id2, ...]}
  - APIs
    - POST /post/create -> post_id
    - POST /user/{user_id}/follow -> success
    - GET  /feed
  - cache (500 posts) for each first time user
    - get only 50 feeds when they scroll down
  - [High Level Design](./11.Simple_Solution.png)
    - Create Post
    - follow people
    - get feed of posts from the people they follow
      1. get all the followees from graph DB
      2. get posts from each followee
      3. add feed to the cache
  - Problem
    - 1 user can follow many people
    - these people can have a lot of posts
    - set of posts might be huge
  - [Improved Design](./11.Improved_Solution.png)
    - update the feed table when a new post is created
      1. get all the followers from graph DB
      2. add all the followers to a kafka queue 
      3. worker can consume the queue and update the feed table
  - problem
    - inactive users' feed is also geting updated
    we can combine the two approaches tier1: cache | tier2: DB | tier3: real-time computation 


- 1-12. CHAT SYSTEM
  - 1. Requirements
    - Functional
      - æ¶ˆæ¯å‘é€ä¸æ¥æ”¶ï¼š
        ç”¨æˆ·å¯ä»¥é€šè¿‡ Chat Servers å‘é€å’Œæ¥æ”¶å®æ—¶æ¶ˆæ¯ï¼Œç³»ç»Ÿåº”ç¡®ä¿æ¶ˆæ¯èƒ½å¿«é€Ÿé€è¾¾ç›®æ ‡ç”¨æˆ·
      - ç”¨æˆ·åœ¨çº¿çŠ¶æ€ç®¡ç†ï¼š
        Presence Servers è´Ÿè´£è¿½è¸ªç”¨æˆ·çš„åœ¨çº¿/ç¦»çº¿çŠ¶æ€ï¼Œä½¿èŠå¤©ç³»ç»Ÿèƒ½å¤Ÿæ˜¾ç¤ºç”¨æˆ·æ˜¯å¦åœ¨çº¿ã€‚
      - æ¶ˆæ¯é€šçŸ¥ï¼š
        å½“ç”¨æˆ·ç¦»çº¿/ä¸Šçº¿æ—¶ï¼ŒNotification Servers åº”é€šè¿‡é€šçŸ¥ï¼ˆå¦‚æ¨é€é€šçŸ¥ï¼‰å‘ŠçŸ¥ç”¨æˆ·æœ‰æ–°æ¶ˆæ¯ã€‚
    - Non-Functional
      - é«˜å¯ç”¨æ€§ï¼ˆHigh Availabilityï¼‰ï¼š
        æ‰€æœ‰å…³é”®ç»„ä»¶ï¼ˆAPI serversã€Chat serversã€KV stores ç­‰ï¼‰éƒ½åº”å…·å¤‡å†—ä½™éƒ¨ç½²ï¼Œé¿å…å•ç‚¹æ•…éšœã€‚
      - ä½å»¶è¿Ÿï¼ˆLow Latencyï¼‰ï¼š
        æ¶ˆæ¯åº”ä»¥æ¯«ç§’çº§å»¶è¿Ÿå®Œæˆä¼ è¾“ï¼Œç‰¹åˆ«æ˜¯ WebSocket é€šé“ä¸‹çš„å®æ—¶é€šä¿¡ã€‚
      - å¯æ‰©å±•æ€§ï¼ˆScalabilityï¼‰ï¼š
        æ¶æ„åº”æ”¯æŒæ¨ªå‘æ‰©å±•ï¼Œèƒ½å¤Ÿåº”å¯¹æ•°ç™¾ä¸‡ç”¨æˆ·å¹¶å‘è¿æ¥å’Œå¤§é‡æ¶ˆæ¯æµé‡ï¼ˆé€šè¿‡åˆ†å¸ƒå¼ KV store å’Œå¯æ‰©å±•æœåŠ¡å™¨æ± å®ç°ï¼‰ã€‚
  - 2. Entities
    - User
    - Message (1-1)
        | message      |
        |--------------|
        | message_id   |
        | message_from |
        | message_to   | 
        | content      |
        | timestamp    |

    - Message (Group)
        | message      |
        |--------------|
        | channel_id   |
        | message_id   |
        | user_id      |
        | content      |
        | timestamp    |
      - message_id
        - Chapter 7: Design a unique ID generator in a distributed system
          - unique
          - chronological
            |  41 bits  |    5 bits     |   5 bits   |     12 bits     |
            |-----------|---------------|------------|-----------------|
            | Timestamp | Datacenter ID | Machine ID | Sequence number |
            | Millisecs |  32 centers   | 32 machine |reset millisecond|
        - id is only unique within a group
  - 3. APIs
    - POST /message/send -> message_id
    - GET  /message/{user_id} -> message_list
    - GET  /message/{group_id} -> message_list
    - Group chat vs 1-on-1 chat
    - send / receive message
      sender ---alive http--> chat service ---???method???--> receiver 
      - polling: continuously check for new messages
      - long polling: receiver sends request and server holds it until new message arrives
        - may not connect to the same server
        - don't know if the receiver is disconnected 
        - inefficient, still make connections
      - WebSocket: bi-directional and persistent
        - sender <--WebSocket--> chat service <--WebSocket--> receiver
  - [High Level Design](./12.chat%20system.png)
    - Presence servers: manage online/offline status 
    - API servers: handle everything including user login, signup, change profile, etc.
    - Notification servers: send push notifications.
    - KV store: chat history
  - storage 
    - Chat history: KV store
    - user profile, friends list: DB, graph DB
  - Dive Deep
    - Service discovery
      - [zookeeper](./12.zookeeper.png)
        recommends the best server bassed on location, capacity .etc.
  - [message flow](./12.message_flow.png)
    3. Chat server 1 sends the message to the message sync queue.
    4. The message is stored in a key-value store.
    5.b. If User B is offline, a push notification is sent from Push Notification (PN) servers.
  - [Message synchronization across multiple devices](./12.sync_flow.png)
    - make use of current message_id
  - [Online presence](./12.Online-off-line.png)
    - login/logout = online/offline
    - heartbeat
  - online statue fanout
    - publish-subscribe model
     

- 13. AUTOCOMPLETE SYSTEM
  data gathering ï½œ query service ï½œ
  - other names: typeahead / design top k
  - éœ€æ±‚åˆ†æ: 
    - QPS
    - latency 100ms
    - top 5 most frequent
  - Data gathering service (real-time)
    | Query   | Frequency |
    |---------|-----------|
    | twich   | 100       |
    | twitter | 500       |

    SELECT * FROM tab
    WHERE query LIKE 'prefix%'
    ORDER BY frequency DESC
    LIMIT 5
    - ç¼ºç‚¹ 
      QPS å¤ªå¤šï¼Œslow
      top k ä¸å’‹å˜
  - Data gathering service (late update, weekly)
    - log -> aggregated data -> Trie DB -> Trie cache
    - log format: 
      | Query    |    Time      |
      |----------|--------------|
      | Linkedin | 250201 13:00 |
    - aggregation
      | Query    |    Time      | Frequency |
      |----------|--------------|-----------|
      | Linkedin | 250201       | 120000    |
    - Trie DB
      - trie O(p) + O(c) + O(clogc) <-- slow 
          (limit)  (cache top query in each node)
      - [trie + cache O(1)](./13.trie%20with%20cache.png)
      - Document store: serialize + MongoDB
      - Key-value store: each node is key
    - Trie cache
      - snapshot of DB
  - [Query Service](./13.Query%20Service.png)
    - AJAX area update (not refresh the whole web page)
    - Browser caching (Google)
    - [Trie delete hateful suggestion](./13.Delete.png)
      also do it when build trie
  -  [Scale the storage (trie cannot fit in one server)](./13.Sharding.png)
    - sharding by first char or first two char ... 
  - follow up
    - multiple languages
      Unicode
    - top search queries different in countries
      different tries and CDNs
    - trending (real-time) search queries


- 4. Distributed Message Queue: kafka
  collecte big data and real-time analysis
  - queue model
    - 1-to-1
  - publish-subscribe model
    - 1-to-many
  - Deep Dive
    - a topic can be partitioned into multiple brokers
      topic1
      | broker 1    | broker 2    | broker 3    | 
      | partition 1 | partition 2 | partition 3 |
    - how to find a message? 
      - topic + partition + offset 
      - topic + key       + offset
    - dumb broker / smart consumer
    - replica (leader + follower)
      - in-sync replica can become leader (min ISRs = 3)
        - high-water mark (at least 3 replicas have the same message )
          - expose to consumer 
  - Consumer Groups (act like a single consumer for a topic)
    - each consumer in a group can read from one partition
    - consumer# = partition#
  - Split brain
    - Generation clock
  - producer asyc/leader/leader+Quorum
  - consumer at-most-once/at-least-once/exactly-once




- 5. Metrics Monitoring and Alerting System
  write heavily: 10 million operational metrics are written
  read spikingly
  - Data collection
  - Data transmission
  - Data storage
    - time series database
      - InfluxDB / Prometheus
  - Alerting
  - Visualization
  - [High-level design](./5.high_level_design.png)
  - Metrics Source --> Metrics Collectors auto-scaling
    - |              |    pull      |   push     |
      |--------------|--------------|------------|
      | debug        | /metrics     |            |
      | Health check | /metrics     | å¯èƒ½ç½‘ç»œé—®é¢˜ |
      | Short jobs   |              | pull é—´éš”é•¿ |
      | Firewall     | å¯èƒ½ä¸èƒ½è®¿é—®   | LB + group |
      | Performance  | TCP          | UDP        |
      | Firewall     | å¯èƒ½ä¸èƒ½è®¿é—®   | LB + group |
      | authenticity | guaranteed   |            |
  - Metrics Collectors --> Time series DB (what if DB is down)
    metrics transmission pipeline
    Metrics Collectors --> kafka --> consumer --> Time series DB
  - Time series DB <-- Query Servers (+ Cache)
  - [alert_system](./5.alert_system.png)

- 2-6. Ad Click Event Aggregation (top-k)
  - requirements
    - functional
      - Aggregate the number of clicks of ad_id in the last M minutes.
      - Return the top 100 most clicked ad_id every minute.
      - Support aggregation filtering by different attributes.
    - non-functional
      - Facebook or Google volume
      - aggregation result is accurate
      - handle delayed or duplicate events
      - Latency should be a few minutes
  - Entities
    - clicks | most 100 clicked
  - APIs
    - GET aggregated_count/{:ad_id} -> count (in the last M min)
    - GET top_100/                  -> LIST: [ad_ids] (in the last M min)
    - raw data (first message queue)
      | ad_id |     timestamp      | user_id | location |
      |-------|--------------------|---------|----------|
      | 12345 | 2025-04-01 00:00:01| 1       | CA       |
      | 12345 | 2025-04-01 00:00:03| 2       | WA       |
      | 22222 | 2025-04-01 00:00:01| 3       | CA       |
    - aggregated data (second message queue)
      | ad_id |     timestamp      | count |
      |-------|--------------------|-------|
      | 12345 | 2025-04-01 00:00   | 2     |
      | 22222 | 2025-04-01 00:00   | 1     |
    - top 100 (second message queue)
      | update_time_minute | most_clicked_ads |
      |--------------------|------------------|
  - [high-level design](./2-6.top_k_every_min.png)
    streaming: near real-time
    - [Aggregation service](./2-6.Aggregation_service.png)
  - Dive Deep
    - [Streaming vs batching](./2-6.replay.png)
    - Delayed events: read from kafka for 5 more seconds
    - Aggregation window
      - 1 minute  fixed window
      - M minutes sliding window 
    - Data deduplication
      - [aggragation service outage](./2-6.duplicated_events.png)
        - [distributed transaction](./2-6.Distributed_trans.png)
    - scalability
      - kafka
        - partitioning (pre-allocate enough partitions)
          - Hashing ad_id as key to make sure the same ad_id is sent to the same partition
        - Add Brokers / Consumers & Rebalance
        - Hashing key (ad_id) find in the same partition
      - [Aggregation service](./2-6.scale_aggregation_service.png)
        - Flink intead of map-reduce
          - stream aggregation
          - aggregation window  1 min
          - flush intervals    10 seconds
      - database
        - sharding by ad_id
        - consistent hashing
    - hotpot for popular ad_id
      - [auto scaling the aggregation node](./2-6.auto_scale_aggregation_node.png)
      - before go to kafka add [0-N + ad_id] to the shard_id (or key in the kafka topic)
    - Fault tolerance
      - snapshot recovery from the last time they read from kafka (S3)
      - no worries about the kafka (it is already fault-tolerant and highly available) 
    - data reconciliation using the batch processing
    


- 2-10. Real-time Gaming Leaderboard
  - [high level design](./2-10.high_level_design.png)
  - workflow                        -> Analytic Service
    - user -> game service -> kafka -> Leaderboard Service(win ? +1 : -1)
                                    -> Push Notification Service
  - âŒ Database slow
  - âœ… [redis sorted set](./2-10.Redis_skipped_list.jpg)
    - O(log n)

  -                       -> Leaderboard sorted set 
  - user -> load balancer -> User profile + User points
                          -> Top 10 user cache
  - redis sharding
    [top 1-100] - [top 101-200] - [...] ...



- LinkedIn Job Recommendation System (1-11. NEWS FEED SYSTEM)
  show recommendation after user open app
  should display at least 5 jobs per reasons, reason could be anything from
  your profile (location, education, industry, etc.)
  - Functional requirements
    - reason
      - Location
      - post by followee
      - skill set
      - frequent item set
      - popularity

  - Non-functional requirements
  - Capacity Estimation
  - Workflow
    - user A add post
    - user B,C 
      - getUserFeed()
      - pushFeedToUser() (Long Poll)
  - cache + DB1
    - User è¡¨
    - Entity è¡¨
    - UserFollow è¡¨
  - cache + DB2
    - FeedItem è¡¨
  - feed cache
    - 1 post by followee
      - {user A: post1, post2, ... 5 at most}
      - user B get all the followers, including user A (graph DB)
      - aggregate all followers' posts
    - 2 location
      - {CA: pos1, pos2..., WA: pos1, pos2...}
    - skill set
      - product of user skill set and job skill set
    - frequent item set / collaborative filtering
      other users applied for this job also applied A, B, C
    - popularity
      - accumulator for each job [10. Real-time Gaming Leaderboard]

- IP é»‘åå•æœåŠ¡
  - è¦è®¾è®¡ä¸€ä¸ª malicious IP blacklist serviceï¼Œå¹¶ä¸”å‡è®¾å·²ç»æä¾›äº†ä¸€ä¸ª isMalicious(ip) API æ¥åˆ¤æ–­æŸä¸ª IP æ˜¯å¦æ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œç³»ç»Ÿè®¾è®¡ï¼š
  - Requirements
    - åŠŸèƒ½æ€§éœ€æ±‚ï¼ˆFunctional Requirementsï¼‰
        â€¢	æ¥æ”¶ IP å¹¶åˆ¤æ–­æ˜¯å¦æ¶æ„ã€‚
    - éåŠŸèƒ½æ€§éœ€æ±‚ï¼ˆNon-Functional Requirementsï¼‰
        â€¢	é«˜å¹¶å‘ã€é«˜å¯ç”¨ï¼ˆé¢å‘äº’è”ç½‘å¼€æ”¾ï¼‰ã€‚
        â€¢	å¿«é€Ÿå“åº”æ—¶é—´ï¼ˆæ¯«ç§’çº§åˆ«æŸ¥è¯¢ï¼‰ã€‚
        â€¢	é«˜å¯æ‰©å±•æ€§ï¼ˆæµ·é‡ IPï¼Œåˆ†å¸ƒå¼éƒ¨ç½²ï¼‰ã€‚
        â€¢	æ•°æ®ä¸€è‡´æ€§ä¸æŒä¹…æ€§ï¼ˆé»‘åå•ä¸èƒ½è½»æ˜“ä¸¢å¤±ï¼‰ã€‚
  - API
  - ğŸ§± 2. é«˜å±‚è®¾è®¡ç»„ä»¶ï¼ˆHigh-Level Designï¼‰
    +-----------------+       +-----------------+       +----------------------+
    |  Client Request |  -->  |  API Gateway    |  -->  |  IP Blacklist Service|
    +-----------------+       +-----------------+       +----------------------+
                                                                |
                                                                v
                          +---------------------------------------------+
                          |     Cache Layer (e.g., Redis / Memcached)   |
                          +---------------------------------------------+
                                                                |
                                                                v
                          +---------------------------------------------+
                          |    Persistent DB (e.g., Cassandra, DynamoDB)|
                          +---------------------------------------------+
                                                                |
                                                                v
                                                    +--------------------+
                                                    | isMalicious(ip) API|
                                                    +--------------------+
  - ğŸ§  3. æ ¸å¿ƒæµç¨‹é€»è¾‘
    - æŸ¥è¯¢ IP æ˜¯å¦æ¶æ„ï¼ˆGET /isBlacklisted?ip=1.2.3.4ï¼‰
      1. æŸ¥è¯¢ç¼“å­˜ï¼ˆRedisï¼‰æ˜¯å¦å­˜åœ¨è¯¥ IP çš„çŠ¶æ€ï¼š
        â€¢	å‘½ä¸­ï¼šç›´æ¥è¿”å›ç»“æœã€‚
        â€¢	æœªå‘½ä¸­ï¼š
        â€¢	è°ƒç”¨ isMalicious(ip)ã€‚
        â€¢	è‹¥è¿”å› trueï¼Œåˆ™åŠ å…¥ç¼“å­˜å’ŒæŒä¹…å±‚é»‘åå•è¡¨ä¸­ã€‚
        â€¢	è¿”å›æŸ¥è¯¢ç»“æœã€‚
      2. åå°æ‰¹é‡æ£€æŸ¥ä»»åŠ¡ corn job
        â€¢	å®šæœŸä»æ•°æ®åº“ä¸­æŠ½æ ·éé»‘åå• IPï¼Œé‡æ–°éªŒè¯æ˜¯å¦å˜æˆ maliciousã€‚
        â€¢	æ”¯æŒè¿‡æœŸæ¸…ç†æœºåˆ¶ï¼ˆä¾‹å¦‚æŸäº› IP ä¸€æ®µæ—¶é—´åä¸å†è¢«åˆ¤å®šæ¶æ„ï¼‰ã€‚
  - ğŸ—ƒï¸ æ•°æ®å­˜å‚¨è®¾è®¡
    - Redis ç¼“å­˜ç»“æ„
        Key: "blacklist:1.2.3.4"
        Value: true / false
        TTL: 24 hours or configurable
    - æŒä¹…å±‚ï¼ˆNoSQL æˆ–å…³ç³»å‹ï¼‰
        IP Address	IsMalicious	FirstDetected	LastChecked	Source
        1.2.3.4	    true      	2025-04-01	  2025-04-04	isMalicious API
  - ğŸ”’ å®‰å…¨æ€§è®¾è®¡
      â€¢	é™åˆ¶æ¯ç§’æŸ¥è¯¢æ¬¡æ•°ï¼ˆrate limitingï¼‰ã€‚
      â€¢	é˜²æ­¢æ¶æ„ç”¨æˆ·æ‰¹é‡æµ‹è¯•ï¼ˆåŠ è®¤è¯/é£æ§ï¼‰ã€‚
      â€¢	æ—¥å¿—è®°å½•æ‰€æœ‰ API è°ƒç”¨åŠåˆ¤å®šç»“æœã€‚
    â±ï¸ å¯é€‰ä¼˜åŒ–
      â€¢	å¼‚æ­¥æ‰¹é‡éªŒè¯ï¼šå°†æœªçŸ¥ IP æ‰”å…¥æ¶ˆæ¯é˜Ÿåˆ—ä¸­ï¼Œç”± worker å¼‚æ­¥è°ƒç”¨ isMalicious(ip)ã€‚
      â€¢	åˆ†å¸ƒå¼ç¼“å­˜ï¼šä½¿ç”¨ Redis Cluster å®ç°æ¨ªå‘æ‰©å±•ã€‚
      â€¢	ç°åº¦æ›´æ–°æœºåˆ¶ï¼šå¯¹æŸäº› IP æ£€æŸ¥ç»“æœæ¨¡ç³Šçš„æƒ…å†µä½¿ç”¨æŠ•ç¥¨æœºåˆ¶ã€æœºå™¨å­¦ä¹ åˆ¤æ–­ç­‰ã€‚
      


- 3-3. Ticket Master
  - Functional Requirements
    - æŸ¥çœ‹å‰©ä½™ç¥¨æ•°
    - è´­ä¹°ç¥¨ å¯ä¸€æ¬¡å¤šå¼  GA (General Admission)
  - Non-Functional Requirements
    - High Availability
    - Consistency
  - Entities
    - Event
        event_id
        total_tickets
        remaining_tickets
    - order
        order_id
        event_id
        ticket_count
        user_id
  - APIs
    - GET /events/{event_id} -> remaining_tickets
    - POST /purchase?event_id=123&ticket_count=2 â€”> order_id
  - [High-Level Design](./3-3.High_level.png)
  - Problem: 
    reserve tickets for 10 min when user clicks purchase
    - distributed lock 
      - Redis Lua åœ¨å•èŠ‚ç‚¹å†…å­˜ä¸­åšåŸå­æ‰£å‡ with TTL
      ä¸€æ—¦ Redis é¢„å æˆåŠŸ + ä»˜æ¬¾æˆåŠŸ
      1.	ç”Ÿæˆè®¢å•æ¶ˆæ¯ï¼ŒåŒ…å« userId, eventId, wantCount, requestId
      2.	æŠ•é€’åˆ°æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆKafka / RabbitMQ / SQSï¼‰
    - OCC (Optimistic Concurrency Control)
      ä¸€æ¬¡è´­ç¥¨éœ€è¦åœ¨å­˜å‚¨å±‚åŒæ—¶å®Œæˆä¸¤ä»¶äº‹ï¼š
        1.	æ‰£å‡åº“å­˜ï¼šticket.inventory = ticket.inventory - Nï¼Œä¸”è¦æ±‚æ‰§è¡Œå inventory â‰¥ 0ã€‚
        2.	å†™å…¥è®¢å•ï¼šåœ¨ order è¡¨æ’å…¥ä¸€è¡Œ { user_id, ticket_id, count=N }ã€‚
        å…³é”®åœ¨äºï¼šä¸¤æ­¥å¿…é¡»è¦ä¹ˆéƒ½æˆåŠŸã€è¦ä¹ˆéƒ½ä¸ç”Ÿæ•ˆï¼Œå³åŸå­æ€§ã€‚
      while (true) 
        UPDATE Event SET remaining_tickets = ?, version = [[old_version+1]]
         WHERE event_id = ? AND version = [[old_version]]
        rollback or update Order
    - ä»˜æ¬¾ä¸æˆåŠŸ 
      Keyspace Notification ç›‘å¬è¿‡æœŸäº‹ä»¶
      Lua é‡Šæ”¾è„šæœ¬
    - spike elimination
      kafka 
  - [Improvement](./3-3.improvement.png)






- 3-4. Live Commenting System
  - Functional Requirements
    - post comment
    - view new comments
    - view history comments 
  - Non-Functional Requirements
    - support millions of concurrent videos &&
      thousands of comments per second per live video.
    - availability >> consistency
    - low latency near-real time < 200ms
  - Entities
    - comment
    - video 
    - user
  - APIs
    - POST /comments/:liveVideoId
    - GET  /comments/:liveVideoId ? cursor={last_comment_id}
    - get comments
      comment service ---???method???--> receiver 
      - polling: continuously check for new messages
      - long polling: receiver sends request and server holds it until new message arrives
        - may not connect to the same server
        - don't know if the receiver is disconnected 
        - inefficient, still make connections
      - WebSocket: bi-directional and persistent
        - comment service <--WebSocket--> receiver
        - non-http protocol
        - all infra needs to support WebSocket  [client] <-> [API gateway AWS] <-> [comment service]
      - âœ… SSE (Server-Sent Events) (200ms)
        - unidirectional
        - HTTP 
        - built-in reconnection by browser
        - light weight
  - [High-level Design](./3-4.Live_Comment.png)
    - POST å†™å…¥ â†’ Comment Service â†’ Kafka
	  - Kafka partition (åŸºäº videoId çš„ rendezvous hash) â†’ å¯¹åº” fan-out å®ä¾‹
	  - SSE è®¢é˜…ï¼ˆåŒæ ·ç”¨ videoId çš„ rendezvous hashï¼‰â†’ è·¯ç”±åˆ°åŒä¸€å®ä¾‹
	  - fan-out å®ä¾‹ åªæ¶ˆè´¹è‡ªå·±é‚£ä»½ partitionï¼Œå¹¶ç»´æŠ¤é‚£æ‰¹ SSE è¿æ¥ â†’ æ¨é€æ–°è¯„è®º
      - ä¿è¯ SSE è·¯ç”±å±‚å’Œ Kafka Producerï¼ˆ/Consumerï¼‰çœ‹åˆ°çš„ã€Œæ´»è·ƒèŠ‚ç‚¹åˆ—è¡¨ã€ä¸€è‡´ï¼Œå¹¶ç”¨åŒä¸€ä¸ª Rendezvous Hash å®ç°æ˜ å°„ã€‚
        - zookeeper 
        - kafka admin



- 3-5 YouTube Top K Videos
  - Requirements
    - Functional requirements
      - Query TopK videos (max 1000)
      - 1 hour, 1 day, 1 month, all-time
    - Non-Functional requirements
      - < 1 min delay between when a è§‚çœ‹ occurs and when it should be tabulated
      - No approximations
      - get result fast < 100ms
  - Entities
    video
    view
    window -> Minute/Hour/Day/All-time
  - APIs 
    - input [video_id, timestamp]
    - GET /views/top?window={WINDOW}&k={K}
  - [High-Level Design](./3-5.high_level_design.png)
  - Deep Dive
    - How to calculate hour count / day count?
      - [two pointers](./3-5.two_pointers.png)
        when decrementing / incrementing the count, heap should be shiftDown / bubbleUp
    - client get top k
      - cache with TTL 40s (allow < 1 min delay)
    - çŠ¶æ€æŒä¹…åŒ–ä¸æ¢å¤
      æ¯ä¸ª Counter å®ä¾‹æœ¬åœ°çŠ¶æ€ï¼ˆcounts + heapï¼‰å®šæœŸ snapshot åˆ°æŒä¹…åŒ–å­˜å‚¨ï¼ˆBlobã€RocksDB changelogï¼‰ï¼›
      é‡å¯æˆ–æ–°å¢å®ä¾‹å¯ä»¥å¿«é€Ÿä»æœ€è¿‘ä¸€æ¬¡ snapshot + Kafka replay æ¢å¤è®¡æ•°ï¼Œä¸ä¸¢æ•°æ®ã€‚
    - hot spot
      å¯¹äºæ’­æ”¾é‡è¶…çº§é«˜çš„â€œçˆ†æ¬¾â€è§†é¢‘ï¼Œå¯åœ¨å“ˆå¸Œåå†åšäºŒçº§åˆ†ç‰‡ï¼ˆç»™åŒä¸€ä¸ª videoId å¤šä¸ª shard keyï¼‰ï¼Œå¤šè·¯å†™å…¥åèšåˆå®ƒä»¬çš„å­è®¡æ•°ï¼›
      è¿™æ ·é¿å…å•ä¸ª Counter æˆä¸ºç“¶é¢ˆã€‚
    - Min Sketch
      Bloom Filter åªæ˜¯å‘Šè¯‰ä½ â€œæœ‰æ²¡æœ‰è§è¿‡ Xï¼Ÿâ€
      Min Sketch   å‘Šè¯‰ä½ â€œå¤§æ¦‚è§è¿‡å¤šå°‘æ¬¡ Xï¼Ÿâ€

- 3-6 LeetCode
  - Functional Requirements
    - view a list of coding problems.
    - view a given problem, code a solution
    - submit  solution and get instant feedback.
    - view a live leaderboard for competitions.
  - Non-Functional Requirements
    - availability > consistency.
    - support isolation and security when running user code.
    - eturn submission results within 5 seconds.
    - scale to support competitions with 100,000 users.
  - Entities 
    - Problem
    - Submission
    - Leaderboard
  - APIs
    - GET /problems?page=1&limit=100 -> Partial<Problem>[]
    - GET /problems/:id?language={language} -> Problem
    - POST /problems/:id/submit -> Submission
    - GET /leaderboard/:competitionId?page=1&limit=100 -> Leaderboard
  - [High-Level Design](./3-6.high_level_design.png)
    - DB å‰é¢å¯ä»¥åŠ ä¸ª cache
  - Deep Dive
    - how to run test cases?
      - serialize the test cases
      - å„è¯­è¨€ deserialize && compare 
    - how to support 100,000 users? 
      - Horizontal auto-scaling w/ Queue
        async but polling (GET /check/:id)


- 3-7 Netflix / YouTube
  - Requirements
    - Functional requirements
      1. Users can upload videos.
      2. Users can watch (stream) videos.
    - Non-Functional requirements
      1. availability > consistency
      2. uploading and streaming large videos (10s of GBs).
      3. low latency streaming of videos, even in low bandwidth environments
      4. ~1M videos uploaded per day, 100M videos watched per day
      5. support resumable uploads.
  - Entities
    - video 
    - video metadata
  - APIs 
    - POST /videos   (only for small videos)
    - GET  /videos/:id -> video && metadata 
  - [High-Level Design](./high_level_design.png)
    - [Video Upload](./3-7.video_upload.png)
      1. cut into small segments
      2. get presigned URL from S3
      [client] <-> [API Gateway] <-> [video service] <-> [S3] <-> [video processing service]
                                                     <-> [video metadata DB]  <-|
      3. upload directly to S3
    - [Video Streaming](./3-7.video_streaming.png)
      1. download manifest file (æœ‰å“ªäº›ç ç‡/æ¸…æ™°åº¦å¯é€‰ï¼Œåˆ†ç‰‡çš„ URLã€æ—¶é•¿ã€é¡ºåº)
      2. download the first segment from S3 / CDN
  - Deep Dive
    - hot videos 
      - cache in front of [video metadata DB]
    - resumable uploads
      MD5 + BitMap
    - scaling 
      - Video Service stateless horizontal scaling
      - Video Metadata - Cassandra 
        - consistent hashing + shard by video_id 
        - cache in front of DB 
      - Video Processing Service
        - kafka + more workers
      - S3 
        - shard by video_id
        - CDN (both segments and manifest files)
          å½“ç”¨æˆ·è¯·æ±‚è§†é¢‘ 12345 çš„ Manifest æ—¶ï¼Œè§†é¢‘æœåŠ¡è®¡ç®— shard = hash(12345)%10 = 7
          Manifest é‡Œæ‰€æœ‰åˆ†ç‰‡ URL éƒ½ä»¥ https://videos-7.cdn.example.com/12345/... å¼€å¤´
          å®¢æˆ·ç«¯å¯¹ videos-7.cdn.example.com å‘èµ· DNS è¯·æ±‚ï¼ŒCDN DNS æ ¹æ®ç”¨æˆ·åœ°ç†+è´Ÿè½½ï¼Œè¿”å›æœ€ä¼˜ PoP çš„ IPã€‚
        
- 3-8. Yelp / nearby location
  - Geo-hash / quad-tree