-ç»ƒä¹ å¦‚ä½•å›ç­” System Design é¢˜ç›®

-é¢è¯•æ—¶å¯ä»¥æŒ‰ç…§ 4-6-2 åŸåˆ™ è¿›è¡Œå›ç­”ï¼š
- 4 åˆ†é’Ÿï¼šéœ€æ±‚åˆ†æ
    Clarify éœ€æ±‚ï¼Œç¡®è®¤æ˜¯å¦æœ‰ SLAã€QPS éœ€æ±‚
    è¯†åˆ«æ ¸å¿ƒåŠŸèƒ½å’ŒéåŠŸèƒ½æ€§è¦æ±‚
- 6 åˆ†é’Ÿï¼šæ¶æ„è®¾è®¡
    ç”»å‡ºæ•´ä½“æ¶æ„ (å®¢æˆ·ç«¯ â†’ è´Ÿè½½å‡è¡¡ â†’ æœåŠ¡å±‚ â†’ æ•°æ®å±‚)
    è§£é‡Šå„ä¸ªç»„ä»¶çš„ä½œç”¨
    è®¨è®ºå¯æ‰©å±•æ€§ã€å®¹é”™æ€§
- 2 åˆ†é’Ÿï¼šä¼˜åŒ– & å–èˆ
    è®¨è®º Trade-offs (æ•°æ®ä¸€è‡´æ€§ vs å¯ç”¨æ€§)
    æ–¹æ¡ˆå¯¹æ¯” (SQL vs NoSQL, Monolithic vs Microservices)


- 4. RATE LIMITER
  counting / sliding window / redis hash {userID: (count, startTime)} for 1 hour / consistent hashing / different rate limiters for different APIs
  - purpose
    - DDOS
    - lower the cost
    - VIP for higher rate
    - spike traffic elimination
  - Functional requirements 
    - 15 requests/second user
    - conpatible with cluster settings
  - Non-Functional requirements 
    - high availability (protect from attack
    - low latency (introduce low or no latency
  - [Design](./4.high_level_design.png)
    - Throttling (http 429 too many requests)
      - hard
      - soft
      - dynamic
    - algo1: counting {userID: (count, startTime)}
      CurrentTime â€“ StartTime >= 1 min
      - Atomicity: read-and-then-writeâ€ behavior can create a race condition
        - two requests at the same time can get the same count
        - redis servers with lock in a distributed setup
      - alow twice the limit
      - Mem usage
        userID 8 bytes
        counter 2 bytes
        startTime 2 bytes only store minute and second
        1 million users
        hash table overhead 20 bytes/record
        lock overhead 4 bytes number lock
        1 million * (8+2+2+20+4)bytes = 36 MB
        
    - algo2: sliding window
      - redis sorted set {userID: SortedSet<Time>}
        8 (userID) + (4 (time) + 20 (sorted set overhead)) * 500 request/hour + 20 (hash-table overhead) = 12KB
        * 1 million = 12GB
      - expensive
    - algo3: 1+2
      1 min counter (count, startTime) using Redis Hash and set expire in 1 hour
      at most 60 counters/hour
      8 + (4 + 2 + 20 (Redis hash overhead)) * 60 + 20 (hash-table overhead) = 1.6KB
      * 1 million = 1.6GB
  - Data Sharding and Caching
    - consistent hashing by userID
    URL Shortener; we can have different rate limiter for createURL() and deleteURL() APIs for each user or IP.
    - cache write back later
  - limit by ID or IP
    - IP bad user can limit other good users in the same network
      IPv6 huge number from even one computer
    - ID
      - Login Auth bad user can enter bad password for the good user DoS
    - IP + ID
      - more memory and storage  

- 6. KEY-VALUE STORE : Dynamo
  Partition | Replication | Consistency(Conflicts) | Scailability | Cluster state
  - CAP ä¸­çš„ A (Available) P (Partition Tolerance)
  - APIs
    - get(key) -> object && context
    - put(key, context, object)
      The context is always stored along with the object and is used like a cookie to verify the validity of the object supplied in the put request.
  - Data partition
    - consistent hashing with virtual nodes
      randomly distributed and non-contiguous
      - evenly distributed && rebalancing fast
      - heterogeneous machines
        lower ranges for less powerful machines
      - no hot spots
  - Replication
    - replication factor 
    - hinted handoff
      æœåŠ¡å™¨æŒ‚äº†ä¹‹åï¼Œreplication é¡ºå»¶ with hint
    - Sloppy quorum
      æ˜¯ä¸€ä¸ªå®¹é”™å†™å…¥æœºåˆ¶ã€‚
      æŒ‡çš„æ˜¯å½“ä¸€éƒ¨åˆ†èŠ‚ç‚¹ä¸å¯ç”¨æ—¶ï¼Œå†™å…¥æ“ä½œå¯ä»¥è¢«æš‚æ—¶å­˜å‚¨åœ¨â€œä¸´æ—¶â€èŠ‚ç‚¹ä¸Šï¼Œç¨åå†åŒæ­¥å›æ­£ç¡®èŠ‚ç‚¹ã€‚
      ç›®çš„æ˜¯æé«˜ç³»ç»Ÿçš„å¯ç”¨æ€§
    - [conflict scenario](./6.Conflict_scenario.png)
  - Vector clock and conflicting data
    - server A and server B
      {A:1} {A:2} {B:1} we only know {A:1} is older
      {A:2} and {B:1} need to be resolved by client
      æˆ–è€… merge {A:2} å’Œ {B:1} çš„ object
    - Conflict-free replicated data types
    - Last-write-wins
  - data reconciliation
    - Merkle trees, solve the conflict in the background
  - Cluster states
    - gossip protocol and seed nodes
  - follow up: å¦‚ä½•å¤„ç† TTLï¼ˆTime-To-Liveï¼‰çš„åˆ é™¤
    - Lazy Delete + Cron Job
  


- 7. UNIQUE ID GENERATOR IN DISTRIBUTED SYSTEMS
  - éœ€æ±‚åˆ†æ: 
    - unique
    - 64bit
    - 10000 IDs/second
    - increasing by time
  - databases auto_increment
    - multiple data centers ç½‘ç»œå»¶è¿Ÿ æ•°æ®ä¸ä¸€è‡´
    - time å¯èƒ½ä¸æ˜¯æ¥è‡ªåŒä¸€ä¸ªæœåŠ¡å™¨ï¼Œä¸ä¸€å®šæ˜¯é€’å¢çš„
    - æ·»åŠ æœåŠ¡å™¨å‡å°‘æœåŠ¡å™¨éƒ½éš¾åŠ
  - Twitter snowflake approach
      |  41 bits  |    5 bits     |   5 bits   |     12 bits     |
      |-----------|---------------|------------|-----------------|
      | Timestamp | Datacenter ID | Machine ID | Sequence number |
      | Millisecs |  32 centers   | 32 machine |reset millisecond|








- 8. URL SHORTENER
  - éœ€æ±‚åˆ†æ: 
    100 million/day
    write/second = 1160
    read         = 11600
    10 years records     = 100 million/day * 365 * 10 years = 365 billion
    storage for 10 years =  365 billion * 100 bytes = 36.5 TB
    values in short URLs: 0 a A = 62 chars
    length of short URLs: 7 
  - [URL Shortening POST](./8.URL%20Shortening.png)
    - Database format:
      |    ID   | Short URL | Long URL 
      |---------|-----------|----------
      | 12345   |  zn9edcu  | https://en.wikipedia.org/wiki/Systems_design
      | 56644   |  fg8723j  | https://en.wikipedia.org/wiki/Systems
    - hash
      - collision
        - Bloom Filter: if element is a member of the set
    - BASE 62 on unique ID
      - not fixed len
      - ID generator [Chapter 7: Design A Unique ID Generator in Distributed
Systems]
      - unique
  - [URL Redirecting (GET)](./8.URL%20Redirecting.png)
  - follow up 
    - Analytics
    - Database scaling
    - Rate limiter (Chapter 4: Design a rate limiter)

- 9. WEB CRAWLER
  - Requirements
    - Functional
      - Crawl the full web starting from seed urls
      - Extract text data and store
    - Non-functional
      - Fault tolerance 
      - Politeness
      - Scale to 10B pages
      - Efficient to crawl in 5 days
  - API or System Interface
    - Input: Seed URLs to start crawling from.
    - Output: Text data extracted from web pages.
  - Data Flow
    - Take seed URL from frontier and request IP from DNS
    - Fetch HTML from external server using IP
    - Extract text data from the HTML.
    - Store the text data in a database.
    - Extract any linked URLs from the web pages and add them to the list of URLs to crawl.
    - Repeat steps 1-5 until all URLs have been crawled.
  - [High Level Design](./9.high_level_design.png)
  - identify bottleneck 
    - wait for web response
    - saparate crawl and parse
  - [improvement](./9.improvement.png)
    - crawl (distributed DNS/crawling) (DNS cache)
    - parse (on-demand)
  - failed URL retry / backoff 1,2,4,8 seconds
  - MATH how many crawlers do we need?
    - 10B pages / 5 days = 2000 pages/second
    - 1 crawler can crawl 100 pages/second
    - 20 crawlers
  - å»é‡
    - URL å»é‡
      - hash table
    - å†…å®¹å»é‡
      - checksum
    - bloom filter

- 10. NOTIFICATION SYSTEM
  - Requirements
    - Functional
      - å¤šæ¸ é“é€šçŸ¥æ”¯æŒï¼š
        ç³»ç»Ÿåº”æ”¯æŒé€šè¿‡å¤šç§æ¸ é“å‘é€é€šçŸ¥ï¼Œå¦‚ Emailã€SMSã€Push Notificationã€In-App ç­‰ã€‚
      - é€šçŸ¥æŠ•é€’çŠ¶æ€è¿½è¸ªï¼š
        ç³»ç»Ÿåº”èƒ½è®°å½•é€šçŸ¥æ˜¯å¦æˆåŠŸå‘é€ã€å¤±è´¥åŸå› ï¼Œä»¥åŠç”¨æˆ·æ˜¯å¦é˜…è¯»ï¼ˆå¯é€‰ï¼Œå–å†³äºæ¸ é“ï¼‰ã€‚
    - Non-functional
      - é«˜å¯ç”¨æ€§ï¼ˆHigh Availabilityï¼‰ï¼š
        é€šçŸ¥ç³»ç»Ÿåº”è®¾è®¡æˆé«˜å¯ç”¨æ¶æ„ï¼Œç¡®ä¿ä¸ä¼šå› ä¸ºéƒ¨åˆ†æœåŠ¡å®•æœºå¯¼è‡´æ•´ä¸ªé€šçŸ¥é“¾ä¸­æ–­
      - å¯æ‰©å±•æ€§ï¼ˆScalabilityï¼‰ï¼š
        ç³»ç»Ÿéœ€è¦æ”¯æŒæ°´å¹³æ‰©å±•ï¼Œèƒ½åœ¨é«˜å³°æœŸå¤„ç†æ•°ç™¾ä¸‡æ¡é€šçŸ¥è¯·æ±‚ï¼ˆæ¯”å¦‚é»‘äº”ã€åŒåä¸€æ´»åŠ¨æ—¶ï¼‰ã€‚
      - ä½å»¶è¿Ÿï¼ˆLow Latencyï¼‰ï¼š
        ç³»ç»Ÿåº”èƒ½ä¿è¯é€šçŸ¥åœ¨äº‹ä»¶å‘ç”Ÿåå°½å¿«ï¼ˆä¾‹å¦‚ <1 ç§’å†…ï¼‰é€è¾¾ç”¨æˆ·ï¼Œå°¤å…¶æ˜¯å¯¹ Push/In-App é€šçŸ¥ã€‚
  - Entities
    User: user_id, device_id, phone, email
    Notification: notification_id, user_id, type, content
  - APIs
    - POST /notifications/send
      - body: {user_id, type, content}
      - response: {status, notification_id}
    - GET /notifications/status/{notification_id}
      - response: {status, error_message}
  - [High Level Design](./10.high_level_design.png)
    - Cache: User info, device info, notification templates are cached.
    - DB: It stores data about user, notification, settings, etc.
  - [Rate limiting / Retry mechanism / Events tracking (clicks, opens)](./10.improved_design.png)

- 11. NEWS FEED SYSTEM
  - Requirements
    - Functions
      - Create Posts
      - Follow people
      - View feed
    - Non-Functional Reuquirements
      - Posts visible in < 2 minutes (eventually consistent)
      - Posting and viewing posts < 500ms
      - Massive number of users 2B
      - Unlimited followers/follows
  - Entities
    User
    Post
    Follow
    // added afterward feed: {uid : [post_id1, post_id2, ...]}
  - APIs
    - POST /post/create -> post_id
    - POST /user/{user_id}/follow -> success
    - GET  /feed
  - cache (500 posts) for each first time user
    - get only 50 feeds when they scroll down
  - [High Level Design](./11.Simple_Solution.png)
    - Create Post
    - follow people
    - get feed of posts from the people they follow
      1. get all the followees from graph DB
      2. get posts from each followee
      3. add feed to the cache
  - Problem
    - 1 user can follow many people
    - these people can have a lot of posts
    - set of posts might be huge
  - [Improved Design](./11.Improved_Solution.png)
    - update the feed table when a new post is created
      1. get all the followers from graph DB
      2. add all the followers to a kafka queue 
      3. worker can consume the queue and update the feed table
  - problem
    - inactive users' feed is also get updated
    we can combine the two approaches tier1: cache | tier2: DB | tier3: real-time computation 


- 12. CHAT SYSTEM
  - Requirements
    - Functional
      - æ¶ˆæ¯å‘é€ä¸æ¥æ”¶ï¼š
        ç”¨æˆ·å¯ä»¥é€šè¿‡ Chat Servers å‘é€å’Œæ¥æ”¶å®æ—¶æ¶ˆæ¯ï¼Œç³»ç»Ÿåº”ç¡®ä¿æ¶ˆæ¯èƒ½å¿«é€Ÿé€è¾¾ç›®æ ‡ç”¨æˆ·
      - ç”¨æˆ·åœ¨çº¿çŠ¶æ€ç®¡ç†ï¼š
        Presence Servers è´Ÿè´£è¿½è¸ªç”¨æˆ·çš„åœ¨çº¿/ç¦»çº¿çŠ¶æ€ï¼Œä½¿èŠå¤©ç³»ç»Ÿèƒ½å¤Ÿæ˜¾ç¤ºç”¨æˆ·æ˜¯å¦åœ¨çº¿ã€‚
      - æ¶ˆæ¯é€šçŸ¥ï¼š
        å½“ç”¨æˆ·ç¦»çº¿/ä¸Šçº¿æ—¶ï¼ŒNotification Servers åº”é€šè¿‡é€šçŸ¥ï¼ˆå¦‚æ¨é€é€šçŸ¥ï¼‰å‘ŠçŸ¥ç”¨æˆ·æœ‰æ–°æ¶ˆæ¯ã€‚
    - Non-Functional
      - é«˜å¯ç”¨æ€§ï¼ˆHigh Availabilityï¼‰ï¼š
        æ‰€æœ‰å…³é”®ç»„ä»¶ï¼ˆAPI serversã€Chat serversã€KV stores ç­‰ï¼‰éƒ½åº”å…·å¤‡å†—ä½™éƒ¨ç½²ï¼Œé¿å…å•ç‚¹æ•…éšœã€‚
      - ä½å»¶è¿Ÿï¼ˆLow Latencyï¼‰ï¼š
        æ¶ˆæ¯åº”ä»¥æ¯«ç§’çº§å»¶è¿Ÿå®Œæˆä¼ è¾“ï¼Œç‰¹åˆ«æ˜¯ WebSocket é€šé“ä¸‹çš„å®æ—¶é€šä¿¡ã€‚
      - å¯æ‰©å±•æ€§ï¼ˆScalabilityï¼‰ï¼š
        æ¶æ„åº”æ”¯æŒæ¨ªå‘æ‰©å±•ï¼Œèƒ½å¤Ÿåº”å¯¹æ•°ç™¾ä¸‡ç”¨æˆ·å¹¶å‘è¿æ¥å’Œå¤§é‡æ¶ˆæ¯æµé‡ï¼ˆé€šè¿‡åˆ†å¸ƒå¼ KV store å’Œå¯æ‰©å±•æœåŠ¡å™¨æ± å®ç°ï¼‰ã€‚

  - Group chat vs 1-on-1 chat
  - sender ---alive http--> chat service ---???--> receiver 
    - polling: continuously check for new messages
    - long polling: receiver sends request and server holds it until new message arrives
      - may not connect to the same server
      - don't know if the receiver is disconnected 
      - inefficient, still make connections
    - WebSocket: bi-directional and persistent
      - sender <--WebSocket--> chat service <--WebSocket--> receiver
  - [High Level Design](./12.chat%20system.png)
    - Presence servers: manage online/offline status 
    - API servers: handle everything including user login, signup, change profile, etc.
    - Notification servers: send push notifications.
    - KV store: chat history
  - storage 
    - Chat history: KV store
    - user profile, friends list: DB
  - data model
    - 1-on-1 chat
      | message |
      |--------------|
      | message_id   |
      | message_from |
      | message_to   | 
      | content      |
      | timestamp    |

    - group chat
      | message    |
      |------------|
      | channel_id |
      | message_id |
      | user_id    |
      | content    |
      | timestamp  |
    
    - message_id
      - Chapter 7: Design a unique ID generator in a distributed system
        unique
        chronological
      - id is only unique within a group
  - Dive Deep
    - Service discovery
      - [zookeeper](./12.zookeeper.png)
        recommends the best server bassed on location capacity .etc.
  - [message flow](./12.message_flow.png)
    - 5.b. If User B is offline, a push notification is sent from push notification (PN) servers.
  - [Message synchronization across multiple devices](./12.sync_flow.png)
    - make use of current message_id
  - [Online presence](./12.Online-off-line.png)
    - login/logout = online/offline
    - heartbeat
  - online statue fanout
    - publish-subscribe model
     

- 13. AUTOCOMPLETE SYSTEM
  data gathering ï½œ query service ï½œ
  - other names: typeahead / design top k
  - éœ€æ±‚åˆ†æ: 
    - QPS
    - latency 100ms
    - top 5 most frequent
  - Data gathering service (real-time)
    | Query   | Frequency |
    |---------|-----------|
    | twich   | 100       |
    | twitter | 500       |

    SELECT * FROM tab
    WHERE query LIKE 'prefix%'
    ORDER BY frequency DESC
    LIMIT 5
    - ç¼ºç‚¹ 
      QPS å¤ªå¤šï¼Œslow
      top k ä¸å’‹å˜
  - Data gathering service (late update, weekly)
    - log -> aggregated data -> Trie DB -> Trie cache
    - log format: 
      | Query    |    Time      |
      |----------|--------------|
      | Linkedin | 250201 13:00 |
    - aggregation
      | Query    |    Time      | Frequency |
      |----------|--------------|-----------|
      | Linkedin | 250201       | 120000    |
    - Trie DB
      - trie O(p) + O(c) + O(clogc) <-- slow 
          (limit)  (cache top query in each node)
      - [trie + cache O(1)](./13.trie%20with%20cache.png)
      - Document store: serialize + MongoDB
      - Key-value store: each node is key
    - Trie cache
      - snapshot of DB
  - [Query Service](./13.Query%20Service.png)
    - AJAX area update (not refresh the whole web page)
    - Browser caching (Google)
    - [Trie delete hateful suggestion](./13.Delete.png)
      also do it when build trie
  -  [Scale the storage (trie cannot fit in one server)](./13.Sharding.png)
    - sharding by first char or first two char ... 
  - follow up
    - multiple languages
      Unicode
    - top search queries different in countries
      different tries and CDNs
    - trending (real-time) search queries


- 4. Distributed Message Queue: kafka
  collecte big data and real-time analysis
  - queue model
    - 1-to-1
  - publish-subscribe model
    - 1-to-many
  - Deep Dive
    - a topic can be partitioned into multiple brokers
      topic1
      | broker 1    | broker 2    | broker 3    | 
      | partition 1 | partition 2 | partition 3 |
    - how to find a message? 
      - topic + partition + offset 
      - topic + key       + offset
    - dumb broker / smart consumer
    - replica (leader + follower)
      - in-sync replica can become leader (min ISRs = 3)
        - high-water mark (at least 3 replicas have the same message )
          - expose to consumer 
  - Consumer Groups (act like a single consumer for a topic)
    - each consumer in a group can read from one partition
    - consumer# = partition#
  - Split brain
    - Generation clock
  - producer asyc/leader/leader+Quorum
  - consumer at-most-once/at-least-once/exactly-once




- 5. Metrics Monitoring and Alerting System
  write heavily: 10 million operational metrics are written
  read spikingly
  - Data collection
  - Data transmission
  - Data storage
    - time series database
      - InfluxDB / Prometheus
  - Alerting
  - Visualization
  - [High-level design](./5.high_level_design.png)
  - Metrics Source --> Metrics Collectors auto-scaling
    - |              |    pull      |   push     |
      |--------------|--------------|------------|
      | debug        | /metrics     |            |
      | Health check | /metrics     | å¯èƒ½ç½‘ç»œé—®é¢˜ |
      | Short jobs   |              | pull é—´éš”é•¿ |
      | Firewall     | å¯èƒ½ä¸èƒ½è®¿é—®   | LB + group |
      | Performance  | TCP          | UDP        |
      | Firewall     | å¯èƒ½ä¸èƒ½è®¿é—®   | LB + group |
      | authenticity | guaranteed   |            |
  - Metrics Collectors --> Time series DB (what if DB is down)
    metrics transmission pipeline
    Metrics Collectors --> kafka --> consumer --> Time series DB
  - Time series DB <-- Query Servers (+ Cache)
  - [alert_system](./5.alert_system.png)

- 2-6. Ad Click Event Aggregation (top-k)
  - requirements
    - functional
      - Aggregate the number of clicks of ad_id in the last M minutes.
      - Return the top 100 most clicked ad_id every minute.
      - Support aggregation filtering by different attributes.
    - non-functional
      - Facebook or Google volume
      - aggregation result is accurate
      - handle delayed or duplicate events
      - Latency should be a few minutes
  - Entities
    - clicks | most 100 clicked
  - APIs
    - GET aggregated_count/{:ad_id} -> count (in the last M min)
    - GET top_100/ -> LIST: [ad_ids]
  - [high-level design](./2-6.top_k_every_min.png)
    streaming: near real-time
    - [Aggregation service](./2-6.Aggregation_service.png)
  - Dive Deep
    - [Streaming vs batching](./2-6.replay.png)
    - Delayed events: read from kafka for 5 more seconds
    - Aggregation window
      - 1 minute fixed window
      - M minutes sliding window 
    - Data deduplication
      - [aggragation service outage](./2-6.duplicated_events.png)
        - [distributed transaction](./2-6.Distributed_trans.png)
    - scalability
      - kafka
        - partitioning
          - Hashing ad_id as key to make sure the same ad_id is sent to the same partition
      - [Aggregation service](./2-6.scale_aggregation_service.png)
        - Flink intead of map-reduce
          - stream aggregation
          - aggregation window  1 min
          - flush intervals    10 seconds

      - database
        - sharding by ad_id
        - consistent hashing
    - hotpot for popular ad_id
      auto scaling the aggregation node
      before go to kafka add [0-N + ad_id] to the shard_id (or key in the kafka topic)
    - Fault tolerance
      - snapshot recovery from the last time they read from kafka
      - no worries about the kafka (it is already fault-tolerant and highly available) 
    - data reconciliation using the batch processing
    


- 10. Real-time Gaming Leaderboard
  - [high level design](./10.high_level_design.png)
  - workflow                        -> Analytic Service
    - user -> game service -> kafka -> Leaderboard Service(win ? +1 : -1)
                                    -> Push Notification Service
  - âŒ Database slow
  - âœ… [redis sorted set](./10.Redis_skipped_list.jpg)
    - O(log n)

  -                       -> Leaderboard sorted set 
  - user -> load balancer -> User profile + User points
                          -> Top 10 user cache
  - redis sharding
    [top 1-100] - [top 101-200] - [...] ...



- LinkedIn Job Recommendation System
  show recommendation after user open app
  should display at least 5 jobs per reasons, reason could be anything from
  your profile (location, education, industry, etc.)
  - Functional requirements
    - reason
      - Location
      - post by followee
      - skill set
      - frequent item set
      - popularity

  - Non-functional requirements
  - Capacity Estimation
  - Workflow
    - user A add post
    - user B,C 
      - getUserFeed()
      - pushFeedToUser() (Long Poll)
  - cache + DB1
    - User è¡¨
    - Entity è¡¨
    - UserFollow è¡¨
  - cache + DB2
    - FeedItem è¡¨
  - feed cache
    - 1 post by followee
      - {user A: post1, post2, ... 5 at most}
      - user B get all the followers, including user A (graph DB)
      - aggregate all followers' posts
    - 2 location
      - {CA: pos1, pos2..., WA: pos1, pos2...}
    - skill set
      - product of user skill set and job skill set
    - frequent item set / collaborative filtering
      other users applied for this job also applied A, B, C
    - popularity
      - accumulator for each job [10. Real-time Gaming Leaderboard]

- IP é»‘åå•æœåŠ¡
  - è¦è®¾è®¡ä¸€ä¸ª malicious IP blacklist serviceï¼Œå¹¶ä¸”å‡è®¾å·²ç»æä¾›äº†ä¸€ä¸ª isMalicious(ip) API æ¥åˆ¤æ–­æŸä¸ª IP æ˜¯å¦æ¶æ„ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œç³»ç»Ÿè®¾è®¡ï¼š
  - Requirements
    - åŠŸèƒ½æ€§éœ€æ±‚ï¼ˆFunctional Requirementsï¼‰
        â€¢	æ¥æ”¶ IP å¹¶åˆ¤æ–­æ˜¯å¦æ¶æ„ã€‚
    - éåŠŸèƒ½æ€§éœ€æ±‚ï¼ˆNon-Functional Requirementsï¼‰
        â€¢	é«˜å¹¶å‘ã€é«˜å¯ç”¨ï¼ˆé¢å‘äº’è”ç½‘å¼€æ”¾ï¼‰ã€‚
        â€¢	å¿«é€Ÿå“åº”æ—¶é—´ï¼ˆæ¯«ç§’çº§åˆ«æŸ¥è¯¢ï¼‰ã€‚
        â€¢	é«˜å¯æ‰©å±•æ€§ï¼ˆæµ·é‡ IPï¼Œåˆ†å¸ƒå¼éƒ¨ç½²ï¼‰ã€‚
        â€¢	æ•°æ®ä¸€è‡´æ€§ä¸æŒä¹…æ€§ï¼ˆé»‘åå•ä¸èƒ½è½»æ˜“ä¸¢å¤±ï¼‰ã€‚
  - API
  - ğŸ§± 2. é«˜å±‚è®¾è®¡ç»„ä»¶ï¼ˆHigh-Level Designï¼‰
    +-----------------+       +-----------------+       +----------------------+
    |  Client Request |  -->  |  API Gateway    |  -->  |  IP Blacklist Service|
    +-----------------+       +-----------------+       +----------------------+
                                                                |
                                                                v
                          +---------------------------------------------+
                          |     Cache Layer (e.g., Redis / Memcached)   |
                          +---------------------------------------------+
                                                                |
                                                                v
                          +---------------------------------------------+
                          |    Persistent DB (e.g., Cassandra, DynamoDB)|
                          +---------------------------------------------+
                                                                |
                                                                v
                                                    +--------------------+
                                                    | isMalicious(ip) API|
                                                    +--------------------+
  - ğŸ§  3. æ ¸å¿ƒæµç¨‹é€»è¾‘
    - æŸ¥è¯¢ IP æ˜¯å¦æ¶æ„ï¼ˆGET /isBlacklisted?ip=1.2.3.4ï¼‰
      1. æŸ¥è¯¢ç¼“å­˜ï¼ˆRedisï¼‰æ˜¯å¦å­˜åœ¨è¯¥ IP çš„çŠ¶æ€ï¼š
        â€¢	å‘½ä¸­ï¼šç›´æ¥è¿”å›ç»“æœã€‚
        â€¢	æœªå‘½ä¸­ï¼š
        â€¢	è°ƒç”¨ isMalicious(ip)ã€‚
        â€¢	è‹¥è¿”å› trueï¼Œåˆ™åŠ å…¥ç¼“å­˜å’ŒæŒä¹…å±‚é»‘åå•è¡¨ä¸­ã€‚
        â€¢	è¿”å›æŸ¥è¯¢ç»“æœã€‚
      2. åå°æ‰¹é‡æ£€æŸ¥ä»»åŠ¡ corn job
        â€¢	å®šæœŸä»æ•°æ®åº“ä¸­æŠ½æ ·éé»‘åå• IPï¼Œé‡æ–°éªŒè¯æ˜¯å¦å˜æˆ maliciousã€‚
        â€¢	æ”¯æŒè¿‡æœŸæ¸…ç†æœºåˆ¶ï¼ˆä¾‹å¦‚æŸäº› IP ä¸€æ®µæ—¶é—´åä¸å†è¢«åˆ¤å®šæ¶æ„ï¼‰ã€‚
  - ğŸ—ƒï¸ æ•°æ®å­˜å‚¨è®¾è®¡
    - Redis ç¼“å­˜ç»“æ„
        Key: "blacklist:1.2.3.4"
        Value: true / false
        TTL: 24 hours or configurable
    - æŒä¹…å±‚ï¼ˆNoSQL æˆ–å…³ç³»å‹ï¼‰
        IP Address	IsMalicious	FirstDetected	LastChecked	Source
        1.2.3.4	    true      	2025-04-01	  2025-04-04	isMalicious API
  - ğŸ”’ å®‰å…¨æ€§è®¾è®¡
      â€¢	é™åˆ¶æ¯ç§’æŸ¥è¯¢æ¬¡æ•°ï¼ˆrate limitingï¼‰ã€‚
      â€¢	é˜²æ­¢æ¶æ„ç”¨æˆ·æ‰¹é‡æµ‹è¯•ï¼ˆåŠ è®¤è¯/é£æ§ï¼‰ã€‚
      â€¢	æ—¥å¿—è®°å½•æ‰€æœ‰ API è°ƒç”¨åŠåˆ¤å®šç»“æœã€‚
    â±ï¸ å¯é€‰ä¼˜åŒ–
      â€¢	å¼‚æ­¥æ‰¹é‡éªŒè¯ï¼šå°†æœªçŸ¥ IP æ‰”å…¥æ¶ˆæ¯é˜Ÿåˆ—ä¸­ï¼Œç”± worker å¼‚æ­¥è°ƒç”¨ isMalicious(ip)ã€‚
      â€¢	åˆ†å¸ƒå¼ç¼“å­˜ï¼šä½¿ç”¨ Redis Cluster å®ç°æ¨ªå‘æ‰©å±•ã€‚
      â€¢	ç°åº¦æ›´æ–°æœºåˆ¶ï¼šå¯¹æŸäº› IP æ£€æŸ¥ç»“æœæ¨¡ç³Šçš„æƒ…å†µä½¿ç”¨æŠ•ç¥¨æœºåˆ¶ã€æœºå™¨å­¦ä¹ åˆ¤æ–­ç­‰ã€‚
      
# Redis vs Amazon Dynamo ç³»ç»Ÿè®¾è®¡å¯¹æ¯”

## æ¶æ„è®¾è®¡

| ç‰¹æ€§ | Redis | Dynamo |
|------|-------|--------|
| æ¶æ„ | ä¸»ä»æ¶æ„ï¼ˆSentinel æˆ– Cluster æ¨¡å¼ï¼‰ | å»ä¸­å¿ƒåŒ– P2P æ¶æ„ |
| è§’è‰² | æœ‰ä¸»èŠ‚ç‚¹ï¼ˆmasterï¼‰æ§åˆ¶å†™å…¥ | æ‰€æœ‰èŠ‚ç‚¹å¹³ç­‰ï¼Œé€šè¿‡åè°ƒè€…å¤„ç†è¯·æ±‚ |

---

## ä¸€è‡´æ€§æ¨¡å‹

| ç‰¹æ€§ | Redis | Dynamo |
|------|-------|--------|
| ä¸€è‡´æ€§ | å¼ºä¸€è‡´æ€§ï¼ˆä¸»ä»åŒæ­¥ï¼‰æˆ–æœ€ç»ˆä¸€è‡´æ€§ï¼ˆé›†ç¾¤æ¨¡å¼ï¼‰ | æœ€ç»ˆä¸€è‡´æ€§ |
| å†²çªå¤„ç† | ä¸»èŠ‚ç‚¹é¡ºåºå†™å…¥ï¼ŒåŸºæœ¬æ— å†²çª | ä½¿ç”¨ Vector Clock æ£€æµ‹å¹¶åˆå¹¶å†²çª |
| å†™å…¥ | å†™å…¥ä¸»èŠ‚ç‚¹ | å¯å¹¶å‘å†™å…¥å¤šä¸ªå‰¯æœ¬ï¼ˆsloppy quorumï¼‰ |

---

## é«˜å¯ç”¨æ€§ä¸å®¹é”™æœºåˆ¶

| ç‰¹æ€§ | Redis | Dynamo |
|------|-------|--------|
| å®¹é”™æœºåˆ¶ | Sentinel æ•…éšœè½¬ç§»ï¼›Cluster æ¨¡å¼å†…å»ºåˆ‡æ¢ | Hinted Handoff + Gossip åè®® |
| å¤åˆ¶æœºåˆ¶ | ä¸»ä»å¤åˆ¶ï¼ˆå¼‚æ­¥ï¼‰ | N å‰¯æœ¬ï¼Œé‡‡ç”¨ R + W > N çš„ quorum ç­–ç•¥ |
| çƒ­ç‚¹é—®é¢˜ | æœ‰ä¸»èŠ‚ç‚¹å¯èƒ½å¯¼è‡´ç“¶é¢ˆ | ä½¿ç”¨ VNodes åˆ†æ•£è´Ÿè½½ï¼Œé¿å…çƒ­ç‚¹ |

---

## æ•°æ®å­˜å‚¨ä¸æŒä¹…åŒ–

| ç‰¹æ€§ | Redis | Dynamo |
|------|-------|--------|
| å­˜å‚¨æ–¹å¼ | å†…å­˜ä¸ºä¸»ï¼Œå¯æŒä¹…åŒ–ï¼ˆRDB/AOFï¼‰ | ç£ç›˜ä¸ºä¸»ï¼Œå†…å­˜ä¸ºç¼“å­˜ |
| ç”¨é€” | ç¼“å­˜ã€å®æ—¶æ•°æ® | æŒä¹…æ€§ã€é«˜å¯ç”¨å­˜å‚¨ |
| æ•°æ®è§„æ¨¡ | é€‚åˆä¸­å°é›†ç¾¤ | å¯æ‰©å±•è‡³å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿ |

---

## TTL ä¸åˆ é™¤ç­–ç•¥

| ç‰¹æ€§ | Redis | Dynamo |
|------|-------|--------|
| TTL | æ”¯æŒï¼ŒåŸºäºæƒ°æ€§åˆ é™¤å’Œå®šæœŸæ¸…ç† | ä½¿ç”¨ tombstone + åå°æ¸…ç† |
| åˆ é™¤ç­–ç•¥ | Lazy delete + cron job | å¼‚æ­¥ä¼ æ’­ã€ç‰ˆæœ¬æ ‡è®°ï¼ˆtombstoneï¼‰ |

---

## å¸¸è§ä½¿ç”¨åœºæ™¯

| ä½¿ç”¨åœºæ™¯ | Redis | Dynamo |
|----------|-------|--------|
| ç¼“å­˜ | âœ… | âŒ |
| æ’è¡Œæ¦œã€è®¡æ•°å™¨ã€æ¶ˆæ¯é˜Ÿåˆ— | âœ… | âŒ |
| åˆ†å¸ƒå¼é«˜å¯ç”¨å­˜å‚¨ | âš ï¸ | âœ… |
| å¤šæ´»å†™å…¥ï¼ˆè·¨åœ°åŸŸï¼‰ | âŒ | âœ… |
| ç”µå•†è´­ç‰©è½¦ / è®¢å•å­˜å‚¨ | âš ï¸ | âœ… |

---

## æ€»ç»“

- **Redis**ï¼šé«˜æ€§èƒ½å†…å­˜æ•°æ®åº“ï¼Œé€‚åˆç¼“å­˜å’Œå®æ—¶æ€§å¼ºçš„åº”ç”¨ã€‚
- **Dynamo**ï¼šé«˜å¯ç”¨ã€å¯æ‰©å±•çš„åˆ†å¸ƒå¼ Key-Value å­˜å‚¨ç³»ç»Ÿï¼Œæœ€ç»ˆä¸€è‡´æ€§ä¸ºæ ¸å¿ƒè®¾è®¡ç›®æ ‡ã€‚

